# Configuration file for BIGCLAM Breast Cancer Subtype Clustering

# Data paths
data:
  input_file: "data/gse96058_data_target_added.csv"
  output_file: "data/gse96058_reduced_data.csv"
  save_path: "data/BIGCLAM_data/"
  
# Dataset preparation paths
dataset_preparation:
  tcga:
    clinical: "data/brca_tcga_pub_clinical_data.tsv"
    expression: "data/Human__TCGA_BRCA__UNC__RNAseq__HiSeq_RNA__01_28_2016__BI__Gene__Firehose_RSEM_log2.cct.gz"
    output: "data/tcga_brca_data_target_added.csv"
  gse96058:
    clinical: "data/GSE96058_transcript_expression_3273_samples_and_136_replicates.csv"
    expression: "data/GSE96058_gene_expression_3273_samples_and_136_replicates_transformed.csv"
    output: "data/gse96058_data_target_added.csv"
  
# Preprocessing parameters
preprocessing:
  chunk_size: null  # Auto-detect: use 10000 for files >500MB, null for smaller files
  variance_threshold_mode: "mean"  # Fixed after feature-selection refactor; ignored if changed
  correlation_threshold_mode: "mean"
  laplacian_neighbors: 5
  num_selected_features: 2000  # Final genes retained after Laplacian ranking
  
  # Dataset-specific similarity thresholds (optimized via grid search)
  # These ensure fully connected graphs (required for BIGCLAM) with optimal density
  similarity_thresholds:
    tcga_brca_data: 0.1  # TCGA-BRCA: Optimal from grid search (similarity=0.1)
    gse96058_data: 0.4  # GSE96058: Optimal from grid search (similarity=0.4)
    # Default fallback if dataset not specified
    default: 0.4
  
# BIGCLAM parameters
bigclam:
  max_communities: 10  # Maximum communities to search (model finds optimal using AIC/BIC)
  min_communities: 3  # Minimum communities to search (allows discovery of major molecular programs: Luminal, Basal, HER2)
  iterations: 100  # Optimization iterations per community number
  learning_rate: 0.08  # Adam optimizer learning rate
  
  # Dataset-specific model selection criterion
  # BIC (Bayesian Information Criterion): Stronger penalty for complexity, better for smaller datasets
  # AIC (Akaike Information Criterion): Less penalty, better for larger datasets
  model_selection_criterion:
    tcga_brca_data: "BIC"  # TCGA-BRCA: 1,093 samples - use BIC
    gse96058_data: "AIC"   # GSE96058: 3,409 samples - use AIC (BIC penalty too strong)
    default: "BIC"  # Fallback if dataset not specified
  
  # Note: Optimal number of communities is automatically determined using AIC/BIC
  # The model tests min_communities to max_communities and selects the best
  # BIC uses stronger penalty (log(N) × parameters) than AIC (2 × parameters)
  # min_communities=3 allows discovery of major molecular programs (Luminal, Basal, HER2)
  # The model will find the optimal number based on data-driven model selection
  
  # Optimization improvements
  adaptive_lr: true  # Adapt learning rate based on graph size (lower for larger graphs)
  adaptive_iterations: true  # Adapt iterations based on graph size (more for larger graphs)
  early_stopping: true  # Enable early stopping when convergence detected
  convergence_threshold: 1e-6  # Convergence threshold for early stopping
  patience: 10  # Number of iterations without improvement before stopping
  num_restarts: 5  # Number of random restarts per community number (default for grid search consistency)
  
  # Dataset-specific overrides (overrides adaptive defaults)
  dataset_specific:
    tcga_brca_data:
      num_restarts: 5  # TCGA-BRCA: 5 restarts (same as grid search)
      min_communities: 5  # Force 5-community solution for TCGA (matches grid search)
      max_communities: 10
    gse96058_data:
      num_restarts: 5  # GSE96058: 5 restarts (same as grid search)
      min_communities: 4  # Force 4-community solution for GSE96058 (matches grid search)
      max_communities: 10 # AIC used but search range is fixed to k=4
  
# Classifier parameters - Dataset-specific configurations for fine-tuning
classifiers:
  # Default parameters (used if dataset-specific not provided)
  default:
    mlp:
      num_runs: 10
      num_epochs: 10000
      learning_rate: 0.001
      patience: 10
      weight_decay: 0.0001
      dropout_rate: 0.3
      hidden_layers: [80, 50, 20]
      use_augmentation: true  # Enable data augmentation to balance classes
      use_class_weights: true  # Enable class weights for imbalanced classes
      augmentation_noise_std: 0.1  # Standard deviation for Gaussian noise augmentation
      lr_scheduler_factor: 0.8  # Reduce LR by this factor when plateau (0.8 = 20% reduction)
      lr_scheduler_patience: 20  # Wait this many epochs before reducing LR
      lr_scheduler_min_lr: 0.001  # Minimum learning rate (same as initial LR to prevent getting stuck)
    svm:
      kernel: "rbf"
      C: 0.1
      gamma: "scale"
      probability: true
      use_class_weights: true  # Enable class weights for imbalanced classes
  
  # Dataset-specific parameters (fine-tuned for each dataset)
  dataset_specific:
    tcga_brca_data:
      mlp:
        num_runs: 10
        num_epochs: 10000  # Best from tuning
        learning_rate: 0.01  # Best from tuning (increased from 0.001)
        patience: 10
        weight_decay: 0.0001
        dropout_rate: 0.3  # Best from tuning
        hidden_layers: [128, 64, 32, 16]  # Best from tuning (increased capacity)
        use_augmentation: true  # Enable data augmentation to balance classes
        use_class_weights: true  # Enable class weights for imbalanced classes
        augmentation_noise_std: 0.1  # DEPRECATED: Kept for backward compatibility, SMOTE is now used (ignored)
        lr_scheduler_factor: 0.8  # Reduce LR by this factor when plateau (0.8 = 20% reduction)
        lr_scheduler_patience: 20  # Wait this many epochs before reducing LR
        lr_scheduler_min_lr: 0.001  # Minimum learning rate (same as initial LR to prevent getting stuck)
        use_warm_restarts: true  # Use cosine annealing with warm restarts to escape local minima
        warm_restart_T_0: 100  # First restart period (epochs)
        warm_restart_T_mult: 2  # Multiplier for restart period (doubles each time)
        gradient_clip: 1.0  # Gradient clipping to help escape local minima (set to 0 to disable)
      svm:
        kernel: "rbf"  # Best from tuning
        C: 10.0  # Best from tuning (increased from 0.1)
        gamma: "scale"  # Best from tuning
        probability: true
        use_class_weights: true  # Enable class weights for imbalanced classes
    
    gse96058_data:
      mlp:
        num_runs: 10
        num_epochs: 250  # Best from tuning (reduced from 10000)
        learning_rate: 0.0001  # Best from tuning (reduced from 0.001)
        patience: 10
        weight_decay: 0.0001
        dropout_rate: 0.6  # Best from tuning
        hidden_layers: [512, 256, 128, 64]  # Best from tuning (reduced from 6 to 4 layers)
        use_augmentation: true  # Enable data augmentation to balance classes
        use_class_weights: true  # Enable class weights for imbalanced classes
        augmentation_noise_std: 0.1  # DEPRECATED: Kept for backward compatibility, SMOTE is now used (ignored)
        lr_scheduler_factor: 0.8  # Reduce LR by this factor when plateau (0.8 = 20% reduction)
        lr_scheduler_patience: 20  # Wait this many epochs before reducing LR
        lr_scheduler_min_lr: 0.001  # Minimum learning rate (same as initial LR to prevent getting stuck)
        use_warm_restarts: true  # Use cosine annealing with warm restarts to escape local minima
        warm_restart_T_0: 100  # First restart period (epochs)
        warm_restart_T_mult: 2  # Multiplier for restart period (doubles each time)
        gradient_clip: 1.0  # Gradient clipping to help escape local minima (set to 0 to disable)
      svm:
        kernel: "rbf"  # Best from tuning
        C: 10.0  # Best from tuning (increased from 0.1)
        gamma: "scale"  # Best from tuning
        probability: true
        use_class_weights: true  # Enable class weights for imbalanced classes
    
# Data splitting parameters
data_splitting:
  test_size: 0.15
  valid_size: 0.15
  train_size: 0.75
  random_state: 42
  
# Augmentation parameters
augmentation:
  method: "smote"  # SMOTE (Synthetic Minority Oversampling Technique)
  k_neighbors: 5  # Number of nearest neighbors for SMOTE
  random_state: 42  # Random seed for reproducibility
  # Note: noise_mean and noise_std are deprecated (kept for backward compatibility)
  noise_mean: 0.0
  noise_std: 0.1
  
# Output paths
output:
  results_dir: "results/"
  models_dir: "models/"
  visualizations_dir: "results/"

# Grid search parameters (for parameter optimization)
# Ranges are generated automatically from start, end, and step values
# Variance: 0.5 to 15 (step 0.5) = 30 values
# Similarity: 0.1 to 0.9 (step 0.05) = 17 values
# Total combinations: 30 × 17 = 510 per dataset
grid_search:
  tcga:
    similarity_start: 0.1
    similarity_end: 0.9
    similarity_step: 0.1
  gse96058:
    similarity_start: 0.1
    similarity_end: 0.9
    similarity_step: 0.1

